{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "376bc503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "369689f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v):\n",
    "        super().__init__()\n",
    "        self.d_k= d_k\n",
    "        self.d_v= d_v\n",
    "        self.Q_W= nn.Linear(d_model, d_k, bias= False)\n",
    "        self.K_W= nn.Linear(d_model, d_k, bias= False)\n",
    "        self.V_W= nn.Linear(d_model, d_v, bias= False)\n",
    "        self.out_proj= nn.Linear(d_v, d_model, bias= False)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask= None):\n",
    "        # Q, K, V are of shape (batch_size, seq_len, d_model)\n",
    "        # Project to d_k and d_v\n",
    "        Q_proj= self.Q_W(Q) # (batch_size, seq_len_q, d_k)\n",
    "        K_proj= self.K_W(K) # (batch_size, seq_len_k, d_k)\n",
    "        V_proj= self.V_W(V) # (batch_size, seq_len_v, d_v)\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores= torch.matmul(Q_proj, K_proj.transpose(-2, -1)) # Dot product -> Shape: (batch_size, seq_len_q, seq_len_k)\n",
    "        scores= scores / (self.d_k ** 0.5) # Scaling\n",
    "\n",
    "        if mask is not None:\n",
    "            scores= scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights= F.softmax(scores, dim= -1) # (batch_size, seq_len_q, seq_len_k)\n",
    "        output= torch.matmul(attn_weights, V_proj) # (batch_size, seq_len_q, d_v)\n",
    "        output= self.out_proj(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58fb4af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Implementation's Output Shape: torch.Size([2, 3, 4])\n",
      "Torch Implementation's Output Shape: torch.Size([2, 3, 4])\n",
      "Output Difference: 5.960464477539063e-08\n"
     ]
    }
   ],
   "source": [
    "# Setting random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "d_k = 4  # Must equal d_model for single-head attention in PyTorch\n",
    "d_v = 4  # Must equal d_model for single-head attention in PyTorch\n",
    "\n",
    "# Toy input\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Initialize models\n",
    "my_attn = SingleHeadAttention(d_model, d_k, d_v)\n",
    "torch_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=1, bias=False, batch_first=True)\n",
    "\n",
    "# Copy weights from torch imp to my imp before any forward pass\n",
    "combined_weights = torch_attn.in_proj_weight.data  # Shape: [3 * d_model, d_model]\n",
    "Q_W_torch, K_W_torch, V_W_torch = combined_weights.chunk(3)  # Each is [d_model, d_model]\n",
    "\n",
    "my_attn.Q_W.weight.data = Q_W_torch.clone()\n",
    "my_attn.K_W.weight.data = K_W_torch.clone()\n",
    "my_attn.V_W.weight.data = V_W_torch.clone()\n",
    "my_attn.out_proj.weight.data = torch_attn.out_proj.weight.data.clone()  # Copy output projection weight\n",
    "\n",
    "# Compute outputs after copying weights\n",
    "my_output = my_attn(X, X, X)\n",
    "torch_output, _ = torch_attn(X, X, X)\n",
    "\n",
    "print(f\"My Implementation's Output Shape: {my_output.shape}\")\n",
    "print(f\"Torch Implementation's Output Shape: {torch_output.shape}\")\n",
    "print(f\"Output Difference: {torch.abs(my_output - torch_output).max().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a993f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradcheck passes: True\n"
     ]
    }
   ],
   "source": [
    "# Set up for gradcheck\n",
    "input = torch.randn(batch_size, seq_len, d_model, dtype=torch.double, requires_grad=True)\n",
    "my_attn_double = SingleHeadAttention(d_model, d_k, d_v)\n",
    "my_attn_double.to(torch.double)\n",
    "\n",
    "# Test gradcheck\n",
    "def get_attn_out(input):\n",
    "    return my_attn_double(input, input, input)\n",
    "\n",
    "test = torch.autograd.gradcheck(get_attn_out, input, eps=1e-6, atol=1e-4)\n",
    "print(\"Gradcheck passes:\", test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_complete_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
