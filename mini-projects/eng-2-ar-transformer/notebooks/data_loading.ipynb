{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4528486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_from_disk\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9fb7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dirs\n",
    "cwd= os.getcwd()\n",
    "data_dir= os.path.join(cwd, \"../data/\")\n",
    "artifacts_dir= os.path.join(cwd, \"../artifacts/\")\n",
    "src_dir= os.path.join(cwd, \"../src/\")\n",
    "en_path= data_dir + 'UNPC.ar-en.en'\n",
    "ar_path= data_dir + 'UNPC.ar-en.ar'\n",
    "en_path= os.path.abspath(en_path)\n",
    "ar_path= os.path.abspath(ar_path)\n",
    "\n",
    "# Add src to path to import your modules\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from model import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e771402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset_path, en_tokenizer_path, ar_tokenizer_path, max_seq_len= 512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_path: Path to saved Hugging Face dataset\n",
    "            en_tokenizer_path: Path to English tokenizer\n",
    "            ar_tokenizer_path: Path to Arabic tokenizer\n",
    "            max_seq_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        # Load dataset\n",
    "        self.dataset= load_from_disk(dataset_path)\n",
    "\n",
    "        # Load tokenizers\n",
    "        self.en_tokenizer= Tokenizer.from_file(en_tokenizer_path)\n",
    "        self.ar_tokenizer= Tokenizer.from_file(ar_tokenizer_path)\n",
    "\n",
    "        # Get special tokens\n",
    "        self.pad_id= self.en_tokenizer.token_to_id(\"<PAD>\")\n",
    "        self.sos_id= self.en_tokenizer.token_to_id(\"<SOS>\")\n",
    "        self.eos_id= self.en_tokenizer.token_to_id(\"<EOS>\")\n",
    "\n",
    "        self.max_seq_length= max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item= self.dataset[idx]\n",
    "        en_text= item['en']\n",
    "        ar_text= item['ar']\n",
    "\n",
    "        # Tokenize English (source)\n",
    "        en_encoding= self.en_tokenizer.encode(en_text)\n",
    "        en_ids= en_encoding.ids[:self.max_seq_length]\n",
    "\n",
    "        # Tokenize Arabic (target) - add special tokens\n",
    "        ar_encoding= self.ar_tokenizer.encode(ar_text)\n",
    "        ar_ids= [self.sos_id] + ar_encoding.ids[:self.max_seq_length-2] + [self.sos_id]\n",
    "\n",
    "        return {\n",
    "            'en_ids': torch.tensor(en_ids, dtype=torch.long),\n",
    "            'ar_ids': torch.tensor(ar_ids, dtype= torch.long),\n",
    "            'en_text': en_text,\n",
    "            'ar_text': ar_text\n",
    "        }\n",
    "\n",
    "def collate_fn(batch, pad_id):\n",
    "    \"\"\"Custom collate function to pad sequences and create masks\"\"\"\n",
    "    en_ids= [item['en_ids'] for item in batch]\n",
    "    ar_ids= [item['ar_ids'] for item in batch]\n",
    "\n",
    "    # Pad Sequences\n",
    "    en_ids_padded= torch.nn.utils.rnn.pad_sequence(\n",
    "        en_ids, batch_first= True, padding_value= pad_id\n",
    "    )\n",
    "    ar_ids_padded= torch.nn.utils.rnn.pad_sequence(\n",
    "        ar_ids, batch_first= True, padding_value= pad_id\n",
    "    )\n",
    "\n",
    "    # Create masks\n",
    "    en_mask= (en_ids_padded != pad_id).unsqueeze(1).unsqueeze(2) # (batch_size, 1, 1, src_len)\n",
    "    ar_mask= (ar_ids_padded != pad_id).unsqueeze(1).unsqueeze(2) # (batch_size, 1, 1, tgt_len)\n",
    "\n",
    "    # Create causal mask for decoder\n",
    "    tgt_len= ar_ids_padded.size(1)\n",
    "    causal_mask= torch.tril(torch.ones(tgt_len, tgt_len)).bool().unsqueeze(0).unsqueeze(0) # (1, 1, tgt_len, tgt_len)\n",
    "\n",
    "    return {\n",
    "        'en_ids': en_ids_padded,\n",
    "        'ar_ids': ar_ids_padded,\n",
    "        'en_mask': en_mask,\n",
    "        'ar_mask': ar_mask,\n",
    "        'causal_mask': causal_mask,\n",
    "        'en_texts': [item['en_text'] for item in batch],\n",
    "        'ar_texts': [item['ar_text'] for item in batch]\n",
    "    }\n",
    "\n",
    "def create_data_loaders(data_dir_path, tokenizer_path, batch_size= 32, max_seq_length= 512):\n",
    "    \"\"\"\n",
    "    Create Train and validation data loaders\n",
    "    \"\"\"\n",
    "    # Dataset paths\n",
    "    train_ds_path= os.path.join(data_dir_path + 'train_ds')\n",
    "    val_ds_path= os.path.join(data_dir_path + 'val_ds') \n",
    "    # Tokenizer paths\n",
    "    en_tokenizer_path= os.path.join(tokenizer_path + 'bpe_tokenizer_en.json')\n",
    "    ar_tokenizer_path= os.path.join(tokenizer_path + 'bpe_tokenizer_ar.json')\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset= TranslationDataset(train_ds_path, en_tokenizer_path, ar_tokenizer_path, max_seq_length)\n",
    "    val_dataset= TranslationDataset(val_ds_path, en_tokenizer_path, ar_tokenizer_path, max_seq_length)\n",
    "\n",
    "    # Get pad_id from tokenizer\n",
    "    en_tokenizer= Tokenizer.from_file(en_tokenizer_path)\n",
    "    pad_id= en_tokenizer.token_to_id(\"<PAD>\")\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader= DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size= batch_size,\n",
    "        shuffle= True,\n",
    "        collate_fn= lambda batch: collate_fn(batch, pad_id),\n",
    "        num_workers= 4,\n",
    "        pin_memory= True\n",
    "    )\n",
    "    val_loader= DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size= batch_size,\n",
    "        shuffle= False,\n",
    "        collate_fn= lambda batch: collate_fn(batch, pad_id),\n",
    "        num_workers= 4,\n",
    "        pin_memory= True\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8348972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['en_ids', 'ar_ids', 'en_mask', 'ar_mask', 'causal_mask', 'en_texts', 'ar_texts'])\n",
      "English IDs shape: torch.Size([32, 87])\n",
      "Arabic IDs shape: torch.Size([32, 89])\n",
      "English mask shape: torch.Size([32, 1, 1, 87])\n",
      "Causal mask shape: torch.Size([1, 1, 89, 89])\n",
      "\n",
      "Sample English text: 5. The independent expert also met with Mariano Fernández, the Special Representative of the Secretary-General and Head of the United Nations Stabilization Mission in Haiti (MINUSTAH), and with his deputies, Kevin Kennedy and Nigel Fischer. He wishes to thank all the members of their team who provided him with effective support.\n",
      "Sample Arabic text: 5- واجتمع الخبير المستقبل أيضاً بالممثل الخاص للأمين العام ورئيس بعثة الأمم المتحدة لتحقيق الاستقرار في هايتي (بعثة الأمم المتحدة في هايتي)، ماريانو فرناندز، ونائبيه، كيفين كيندي، ونايجل فيشر، ويود أن يتوجه بالشكر إلى جميع أعضاء فريقهما لما قدموه من دعم فعال.\n",
      "Sample English IDs: tensor([   22,    15,   828,  3125,  3507,  1035,  2981,   843, 21796,    78,\n",
      "        21743,    13,   767,  1419,  2397,   773,   767,  1170,    14,   961,\n",
      "          779,  2679,   773,   767,   903,   928, 10410,  1990,   766,  3816,\n",
      "            9,  9300,  1108,   779,   843,  1393, 15953,    13, 30889, 23099,\n",
      "          779, 19545, 14162, 14532,    15,  1485,  5126,   783,  6067,   868,\n",
      "          767,  1457,   773,   981,  3167,  1408,  1499,  3078,   843,  1504,\n",
      "         1101,    15,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1])\n",
      "Sample Arabic IDs: tensor([    2,    22,    14, 18301,  8792,  3364,  2081,  1911,  1638,  1431,\n",
      "         4513,  1212,  6199,  2223,  1293,  1217,  3094,  4861,  1135,  4410,\n",
      "            9,  2223,  1293,  1217,  1135,  4410,  1601,  1471,  4539,   336,\n",
      "        18866,  4700,   319,   299, 12938,  1312,   299,  2047,  2827,  2332,\n",
      "         1352,   299,  8307,   338,  1221,  1135,  1189,   299, 14380,  1148,\n",
      "        38310, 20085,  1169,  1361,  2159,  1461,  1953,  2909, 31472,  1138,\n",
      "         1936,  2950,    15,     2,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader = create_data_loaders(data_dir, artifacts_dir, batch_size=32, max_seq_length=512)\n",
    "\n",
    "# Test the data loader\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch keys: {batch.keys()}\")\n",
    "print(f\"English IDs shape: {batch['en_ids'].shape}\")\n",
    "print(f\"Arabic IDs shape: {batch['ar_ids'].shape}\")\n",
    "print(f\"English mask shape: {batch['en_mask'].shape}\")\n",
    "print(f\"Causal mask shape: {batch['causal_mask'].shape}\")\n",
    "\n",
    "# Sample output\n",
    "sample_idx = 0\n",
    "print(f\"\\nSample English text: {batch['en_texts'][sample_idx]}\")\n",
    "print(f\"Sample Arabic text: {batch['ar_texts'][sample_idx]}\")\n",
    "print(f\"Sample English IDs: {batch['en_ids'][sample_idx]}\")\n",
    "print(f\"Sample Arabic IDs: {batch['ar_ids'][sample_idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_complete_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
